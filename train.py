# -*- coding: utf-8 -*-
"""DL Assignment 1 code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fyswf_IpdEzSg6SzglG6T6TofhJQ_tef
"""

import argparse
import socket
import copy
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import seaborn as sn
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
import wandb


###############################
# Dataset Visualization (Optional)
###############################
def plot_sample_images(X_train, Y_train):
    # Plot one sample image for each class.
    train_labels_to_plot = Y_train
    train_images_to_plot = X_train
    mySet = np.unique(train_labels_to_plot)
    myData = []
    count = 10
    for train_image, train_label in zip(train_images_to_plot, train_labels_to_plot):
        if train_label in mySet:
            index = np.where(mySet == train_label)
            myData.append(train_image)
            mySet = np.delete(mySet, index)
            count -= 1
        if count == 0:
            break
    names = ['Ankle boot', 'T-shirt/top', 'Dress', 'Pullover', 'sneaker',
             'Sandal', 'Trouser', 'Shirt', 'Coat', 'Bag']
    fig, axes = plt.subplots(2, 5, figsize=(5, 5))
    for i, ax in enumerate(axes.flat):
        ax.set_title(names[i])
        ax.imshow(myData[i], cmap=plt.cm.binary)
        ax.axis('off')
    plt.tight_layout()
    plt.show()

###############################
# Activation functions and derivatives
###############################
def activation_function(z, activation_type):
    if activation_type == 'sigmoid':
        return 1 / (1 + np.exp(-np.clip(z, -50, 50)))
    elif activation_type == 'tanh':
        return np.tanh(np.clip(z, -50, 50))
    elif activation_type == 'relu':
        return np.maximum(0, z)
    elif activation_type == 'identity':
        return z
    else:
        raise ValueError("Invalid activation function.")

def activation_derivative(z, activation_type, alpha=0.01):
    if activation_type == 'sigmoid':
        sig = 1 / (1 + np.exp(-np.clip(z, -50, 50)))
        return sig * (1 - sig)
    elif activation_type == 'tanh':
        return 1 - np.tanh(np.clip(z, -50, 50))**2
    elif activation_type == 'relu':
        return np.where(z > 0, 1, alpha)
    elif activation_type == 'identity':
        return np.ones_like(z)
    else:
        raise ValueError("Invalid activation function")

def softmax_func(z):
    eps = 1e-6
    return np.exp(z - np.max(z, axis=0, keepdims=True)) / (np.sum(np.exp(z - np.max(z, axis=0, keepdims=True)), axis=0, keepdims=True) + eps)

###############################
# Parameter Initializer & Accuracy Calculators
###############################
def parameter_intializer(no_of_layers, neurons, parameter_initialize_type):
    W_matrix = []
    b_matrix = []
    nodes_in_prev_layer = 784

    for i in range(no_of_layers):
        W_matrix.append(np.random.randn(neurons, nodes_in_prev_layer))
        if parameter_initialize_type == 'random':
            b_matrix.append(np.random.randn(neurons, 1))
        else:
            b_matrix.append(np.zeros((neurons, 1)))
        nodes_in_prev_layer = neurons

    W_matrix.append(np.random.randn(10, nodes_in_prev_layer))
    if parameter_initialize_type == 'random':
        b_matrix.append(np.random.randn(10, 1))
    else:
        b_matrix.append(np.zeros((10, 1)))
    return W_matrix, b_matrix

def Accuracy_Calculator(no_of_layers, input_data, label, W_matrix, b_matrix, activationFunction, lossFunction):
    a_matrix, h_matrix = forwardPropogation_batch(no_of_layers, input_data, W_matrix, b_matrix, activationFunction)
    output = np.copy(h_matrix[no_of_layers])
    maxi = -1
    max_label = -1
    for i in range(10):
        if output[i] > maxi:
            maxi = output[i]
            max_label = i

    if lossFunction == 'entropy':
        return max_label, -np.log(output[label] + (1e-5))
    else:
        e_of_l = np.zeros((10, 1))
        e_of_l[label] = 1
        return max_label, np.sum((h_matrix[no_of_layers] - e_of_l) ** 2)

def training_Accuracy_Evaluation(no_of_layers, W_matrix, b_matrix, activationFunction, lossFunction):
    training_accuracy_count = 0
    total_training_loss = 0
    # Here, train_images and train_labels are assumed to be global or preloaded
    for training_image, training_label in zip(train_images, train_labels):
        training_image = training_image.reshape(784, 1) / 255.0
        label, accuracy_loss = Accuracy_Calculator(no_of_layers, training_image, training_label, W_matrix, b_matrix, activationFunction, lossFunction)
        total_training_loss += accuracy_loss
        if label == training_label:
            training_accuracy_count += 1
    wandb.log({'training_accuracy_count': training_accuracy_count / 540})
    wandb.log({'training_accuracy_loss': total_training_loss / 54000})
    print("training_accuracy_count :", training_accuracy_count / 540, "training_accuracy_loss :", total_training_loss / 54000)

def validation_Accuracy_Evaluation(no_of_layers, W_matrix, b_matrix, activationFunction, lossFunction):
    validation_accuracy_count = 0
    total_validation_loss = 0
    for validation_image, validation_label in zip(validation_images, validation_labels):
        validation_image = validation_image.reshape(784, 1) / 255.0
        label, accuracy_loss = Accuracy_Calculator(no_of_layers, validation_image, validation_label, W_matrix, b_matrix, activationFunction, lossFunction)
        total_validation_loss += accuracy_loss
        if label == validation_label:
            validation_accuracy_count += 1
    wandb.log({'validation_accuracy_count': validation_accuracy_count / 60})
    wandb.log({'validation_accuracy_loss': total_validation_loss / 6000})
    print("validation_accuracy_count :", validation_accuracy_count / 60, "validation_accuracy_loss :", total_validation_loss / 6000)

###############################
# Forward & Back Propagation
###############################
def forwardPropogation_batch(no_of_layers, training_input_data, W_matrix, b_matrix, activationFunction):
    a_matrix = [0] * (no_of_layers + 1)
    h_matrix = [0] * (no_of_layers + 1)

    if training_input_data.ndim == 3:
        training_input_data = training_input_data.reshape(-1, training_input_data.shape[-1])

    for i in range(no_of_layers):
        if i == 0:
            a_matrix[i] = np.dot(W_matrix[i], training_input_data) + b_matrix[i]
        else:
            a_matrix[i] = np.dot(W_matrix[i], h_matrix[i-1]) + b_matrix[i]
        if activationFunction == 'sigmoid':
            h_matrix[i] = 1 / (1 + np.exp(-a_matrix[i]))
        elif activationFunction == 'tanh':
            h_matrix[i] = np.tanh(a_matrix[i])
        elif activationFunction == 'relu':
            h_matrix[i] = np.maximum(0, a_matrix[i])
        else:
            h_matrix[i] = a_matrix[i]  # Identity

    a_matrix[no_of_layers] = np.dot(W_matrix[no_of_layers], h_matrix[no_of_layers-1]) + b_matrix[no_of_layers]
    h_matrix[no_of_layers] = np.copy(softmax_func(a_matrix[no_of_layers]))
    return a_matrix, h_matrix

def backwardPropogation_batch(no_of_layers, training_input_data, W_matrix, a_matrix, h_matrix,
                              target_classes, activationFunction, lossFunction, batch_size):
    d_of_W = [0] * (no_of_layers + 1)
    d_of_b = [0] * (no_of_layers + 1)
    if target_classes.shape[0] < batch_size:
        batch_size = target_classes.shape[0]
    e_of_l = np.zeros((10, batch_size))
    e_of_l[target_classes, np.arange(batch_size)] = 1
    if lossFunction == 'entropy':
        d_of_a = (h_matrix[no_of_layers] - e_of_l) / batch_size
    else:
        d_of_a = (h_matrix[no_of_layers] - e_of_l) * h_matrix[no_of_layers] * (1 - h_matrix[no_of_layers])
        d_of_a /= batch_size

    for layer in reversed(range(no_of_layers + 1)):
        if layer > 0:
            d_of_W[layer] = np.dot(d_of_a, h_matrix[layer-1].T) / batch_size
        else:
            d_of_W[layer] = np.dot(d_of_a, training_input_data.T) / batch_size
        d_of_b[layer] = np.sum(d_of_a, axis=1, keepdims=True) / batch_size
        if layer > 0:
            d_of_h = np.dot(W_matrix[layer].T, d_of_a)
            if activationFunction == 'sigmoid':
                act_deriv = h_matrix[layer-1] * (1 - h_matrix[layer-1])
            elif activationFunction == 'tanh':
                act_deriv = 1 - h_matrix[layer-1]**2
            elif activationFunction == 'relu':
                act_deriv = (a_matrix[layer-1] > 0).astype(float)
            else:
                act_deriv = 1
            d_of_a = d_of_h * act_deriv
    return d_of_W, d_of_b

###############################
# Optimizers for Gradient Descent
###############################
def SchotasticGD_batch(no_of_layers, neurons, maxiterations, weightDecay, learningRate, batchSize, parameter_initialize_type, activationFunction, lossFunction):
    W_matrix, b_matrix = parameter_intializer(no_of_layers, neurons, parameter_initialize_type)
    num_train = len(train_images)
    for iteration in range(maxiterations):
        indices = np.random.permutation(num_train)
        shuffled_images = train_images[indices]
        shuffled_labels = train_labels[indices]
        for batch_start in range(0, num_train, batchSize):
            batch_end = batch_start + batchSize
            batch_images = shuffled_images[batch_start:batch_end]
            batch_labels = shuffled_labels[batch_start:batch_end]
            batch_input = np.array([img.ravel() for img in batch_images]).T / 255.0
            a_matrix, h_matrix = forwardPropogation_batch(no_of_layers, batch_input, W_matrix, b_matrix, activationFunction)
            d_of_W, d_of_b = backwardPropogation_batch(no_of_layers, batch_input, W_matrix, a_matrix, h_matrix,
                                                       batch_labels, activationFunction, lossFunction, batchSize)
            for i in range(no_of_layers + 1):
                W_matrix[i] = W_matrix[i] - learningRate * d_of_W[i] - weightDecay * W_matrix[i]
                b_matrix[i] = b_matrix[i] - learningRate * d_of_b[i]
        wandb.log({'epoch': iteration})
        print("Iteration :", iteration)
        training_Accuracy_Evaluation(no_of_layers, W_matrix, b_matrix, activationFunction, lossFunction)
        validation_Accuracy_Evaluation(no_of_layers, W_matrix, b_matrix, activationFunction, lossFunction)

def MomentumGD_batch(no_of_layers, neurons, maxiterations, weightDecay, learningRate, batchSize, parameter_initialize_type, activationFunction, lossFunction):
    W_matrix, b_matrix = parameter_intializer(no_of_layers, neurons, parameter_initialize_type)
    previous_W_of_u = [np.zeros_like(w) for w in W_matrix]
    previous_b_of_u = [np.zeros_like(b) for b in b_matrix]
    eta = learningRate
    beta = 0.9
    num_train = len(train_images)
    for iteration in range(maxiterations):
        indices = np.random.permutation(num_train)
        shuffled_images = train_images[indices]
        shuffled_labels = train_labels[indices]
        for batch_start in range(0, num_train, batchSize):
            batch_end = batch_start + batchSize
            batch_images = shuffled_images[batch_start:batch_end]
            batch_labels = shuffled_labels[batch_start:batch_end]
            batch_input = np.array([img.ravel() for img in batch_images]).T / 255.0
            a_matrix, h_matrix = forwardPropogation_batch(no_of_layers, batch_input, W_matrix, b_matrix, activationFunction)
            d_of_W, d_of_b = backwardPropogation_batch(no_of_layers, batch_input, W_matrix, a_matrix, h_matrix,
                                                       batch_labels, activationFunction, lossFunction, batchSize)
            for i in range(no_of_layers + 1):
                curr_W_of_u = beta * previous_W_of_u[i] + d_of_W[i]
                curr_b_of_u = beta * previous_b_of_u[i] + d_of_b[i]
                W_matrix[i] = W_matrix[i] - eta * curr_W_of_u - weightDecay * W_matrix[i]
                b_matrix[i] = b_matrix[i] - eta * curr_b_of_u
                previous_W_of_u[i] = np.copy(curr_W_of_u)
                previous_b_of_u[i] = np.copy(curr_b_of_u)
        wandb.log({'epoch': iteration})
        print("Iteration :", iteration)
        training_Accuracy_Evaluation(no_of_layers, W_matrix, b_matrix, activationFunction, lossFunction)
        validation_Accuracy_Evaluation(no_of_layers, W_matrix, b_matrix, activationFunction, lossFunction)

def NesterovGD_batch(no_of_layers, neurons, maxiterations, weightDecay, learningRate, batchSize, parameter_initialize_type, activationFunction, lossFunction):
    W_matrix, b_matrix = parameter_intializer(no_of_layers, neurons, parameter_initialize_type)
    previous_W_of_u = [np.zeros_like(w) for w in W_matrix]
    previous_b_of_u = [np.zeros_like(b) for b in b_matrix]
    eta = learningRate
    beta = 0.9
    num_train = len(train_images)
    for iteration in range(maxiterations):
        indices = np.random.permutation(num_train)
        shuffled_images = train_images[indices]
        shuffled_labels = train_labels[indices]
        for batch_start in range(0, num_train, batchSize):
            batch_end = batch_start + batchSize
            batch_images = shuffled_images[batch_start:batch_end]
            batch_labels = shuffled_labels[batch_start:batch_end]
            batch_input = np.array([img.ravel() for img in batch_images]).T / 255.0
            a_matrix, h_matrix = forwardPropogation_batch(no_of_layers, batch_input, W_matrix, b_matrix, activationFunction)
            d_of_W, d_of_b = backwardPropogation_batch(no_of_layers, batch_input, W_matrix, a_matrix, h_matrix,
                                                       batch_labels, activationFunction, lossFunction, batchSize)
            for i in range(no_of_layers + 1):
                W_matrix[i] = W_matrix[i] - eta * d_of_W[i] - weightDecay * W_matrix[i]
                b_matrix[i] = b_matrix[i] - eta * d_of_b[i]
                previous_W_of_u[i] = beta * previous_W_of_u[i] + eta * d_of_W[i]
                previous_b_of_u[i] = beta * previous_b_of_u[i] + eta * d_of_b[i]
        wandb.log({'epoch': iteration})
        print("Iteration :", iteration)
        training_Accuracy_Evaluation(no_of_layers, W_matrix, b_matrix, activationFunction, lossFunction)
        validation_Accuracy_Evaluation(no_of_layers, W_matrix, b_matrix, activationFunction, lossFunction)

def rmsProp_batch(no_of_layers, neurons, maxiterations, weightDecay, learningRate, batchSize, parameter_initialize_type, activationFunction, lossFunction):
    W_matrix, b_matrix = parameter_intializer(no_of_layers, neurons, parameter_initialize_type)
    W_of_v = [np.zeros_like(w) for w in W_matrix]
    b_of_v = [np.zeros_like(b) for b in b_matrix]
    eta = learningRate
    beta = 0.5
    eps = 1e-4
    num_train = len(train_images)
    for iteration in range(maxiterations):
        indices = np.random.permutation(num_train)
        shuffled_images = train_images[indices]
        shuffled_labels = train_labels[indices]
        for batch_start in range(0, num_train, batchSize):
            batch_end = batch_start + batchSize
            batch_images = shuffled_images[batch_start:batch_end]
            batch_labels = shuffled_labels[batch_start:batch_end]
            batch_input = np.array([img.ravel() for img in batch_images]).T / 255.0
            a_matrix, h_matrix = forwardPropogation_batch(no_of_layers, batch_input, W_matrix, b_matrix, activationFunction)
            d_of_W, d_of_b = backwardPropogation_batch(no_of_layers, batch_input, W_matrix, a_matrix, h_matrix,
                                                       batch_labels, activationFunction, lossFunction, batchSize)
            for i in range(no_of_layers + 1):
                W_of_v[i] = (1 - beta) * (d_of_W[i] ** 2) + beta * W_of_v[i]
                b_of_v[i] = (1 - beta) * (d_of_b[i] ** 2) + beta * b_of_v[i]
                W_matrix[i] = W_matrix[i] - eta * d_of_W[i] / (np.sqrt(W_of_v[i]) + eps) - weightDecay * W_matrix[i]
                b_matrix[i] = b_matrix[i] - eta * d_of_b[i] / (np.sqrt(b_of_v[i]) + eps)
        wandb.log({'epoch': iteration})
        print("Iteration :", iteration)
        training_Accuracy_Evaluation(no_of_layers, W_matrix, b_matrix, activationFunction, lossFunction)
        validation_Accuracy_Evaluation(no_of_layers, W_matrix, b_matrix, activationFunction, lossFunction)

def adam_batch(no_of_layers, neurons, maxiterations, weightDecay, learningRate, batchSize, parameter_initialize_type, activationFunction, lossFunction):
    W_matrix, b_matrix = parameter_intializer(no_of_layers, neurons, parameter_initialize_type)
    eta = learningRate
    beta1 = 0.9
    beta2 = 0.999
    eps = 1e-10
    num_train = len(train_images)
    W_of_m = [np.zeros_like(W_matrix[i]) for i in range(no_of_layers + 1)]
    b_of_m = [np.zeros_like(b_matrix[i]) for i in range(no_of_layers + 1)]
    W_of_v = [np.zeros_like(W_matrix[i]) for i in range(no_of_layers + 1)]
    b_of_v = [np.zeros_like(b_matrix[i]) for i in range(no_of_layers + 1)]
    update_iteration = 0
    for iteration in range(maxiterations):
        indices = np.random.permutation(num_train)
        shuffled_images = train_images[indices]
        shuffled_labels = train_labels[indices]
        for batch_start in range(0, num_train, batchSize):
            batch_end = batch_start + batchSize
            batch_images = shuffled_images[batch_start:batch_end]
            batch_labels = shuffled_labels[batch_start:batch_end]
            batch_input = np.array([img.ravel() for img in batch_images]).T / 255.0
            a_matrix, h_matrix = forwardPropogation_batch(no_of_layers, batch_input, W_matrix, b_matrix, activationFunction)
            d_of_W, d_of_b = backwardPropogation_batch(no_of_layers, batch_input, W_matrix, a_matrix, h_matrix,
                                                       batch_labels, activationFunction, lossFunction, batchSize)
            update_iteration += 1
            for i in range(no_of_layers + 1):
                W_of_m[i] = (1 - beta1) * d_of_W[i] + beta1 * W_of_m[i]
                b_of_m[i] = (1 - beta1) * d_of_b[i] + beta1 * b_of_m[i]
                W_of_v[i] = (1 - beta2) * (d_of_W[i] ** 2) + beta2 * W_of_v[i]
                b_of_v[i] = (1 - beta2) * (d_of_b[i] ** 2) + beta2 * b_of_v[i]
                W_of_m_cap = W_of_m[i] / (1 - np.power(beta1, update_iteration))
                b_of_m_cap = b_of_m[i] / (1 - np.power(beta1, update_iteration))
                W_of_v_cap = W_of_v[i] / (1 - np.power(beta2, update_iteration))
                b_of_v_cap = b_of_v[i] / (1 - np.power(beta2, update_iteration))
                W_matrix[i] = W_matrix[i] - eta * (W_of_m_cap / (np.sqrt(W_of_v_cap) + eps)) - weightDecay * W_matrix[i]
                b_matrix[i] = b_matrix[i] - eta * (b_of_m_cap / (np.sqrt(b_of_v_cap) + eps))
        wandb.log({'epoch': iteration})
        print("Iteration :", iteration)
        training_Accuracy_Evaluation(no_of_layers, W_matrix, b_matrix, activationFunction, lossFunction)
        validation_Accuracy_Evaluation(no_of_layers, W_matrix, b_matrix, activationFunction, lossFunction)

def nadam_batch(no_of_layers, neurons, maxiterations, weightDecay, learningRate, batchSize, parameter_initialize_type, activationFunction, lossFunction):
    W_matrix, b_matrix = parameter_intializer(no_of_layers, neurons, parameter_initialize_type)
    eta = learningRate
    beta1 = 0.9
    beta2 = 0.999
    eps = 1e-10
    num_train = len(train_images)
    W_of_m = [np.zeros_like(W_matrix[i]) for i in range(no_of_layers + 1)]
    b_of_m = [np.zeros_like(b_matrix[i]) for i in range(no_of_layers + 1)]
    W_of_v = [np.zeros_like(W_matrix[i]) for i in range(no_of_layers + 1)]
    b_of_v = [np.zeros_like(b_matrix[i]) for i in range(no_of_layers + 1)]
    update_iteration = 0
    for iteration in range(maxiterations):
        indices = np.random.permutation(num_train)
        shuffled_images = train_images[indices]
        shuffled_labels = train_labels[indices]
        for batch_start in range(0, num_train, batchSize):
            batch_end = batch_start + batchSize
            batch_images = shuffled_images[batch_start:batch_end]
            batch_labels = shuffled_labels[batch_start:batch_end]
            batch_input = np.array([img.ravel() for img in batch_images]).T / 255.0
            a_matrix, h_matrix = forwardPropogation_batch(no_of_layers, batch_input, W_matrix, b_matrix, activationFunction)
            d_of_W, d_of_b = backwardPropogation_batch(no_of_layers, batch_input, W_matrix, a_matrix, h_matrix,
                                                       batch_labels, activationFunction, lossFunction, batchSize)
            update_iteration += 1
            for i in range(no_of_layers + 1):
                W_of_m[i] = (1 - beta1) * d_of_W[i] + beta1 * W_of_m[i]
                b_of_m[i] = (1 - beta1) * d_of_b[i] + beta1 * b_of_m[i]
                W_of_v[i] = (1 - beta2) * (d_of_W[i] ** 2) + beta2 * W_of_v[i]
                b_of_v[i] = (1 - beta2) * (d_of_b[i] ** 2) + beta2 * b_of_v[i]
                W_of_m_cap = W_of_m[i] / (1 - np.power(beta1, update_iteration))
                b_of_m_cap = b_of_m[i] / (1 - np.power(beta1, update_iteration))
                W_of_v_cap = W_of_v[i] / (1 - np.power(beta2, update_iteration))
                b_of_v_cap = b_of_v[i] / (1 - np.power(beta2, update_iteration))
                W_matrix[i] = W_matrix[i] - (eta / (np.sqrt(W_of_v_cap + eps))) * (beta1 * W_of_m_cap + (1 - beta1) * d_of_W[i] / (1 - np.power(beta1, update_iteration))) - weightDecay * W_matrix[i]
                b_matrix[i] = b_matrix[i] - (eta / (np.sqrt(b_of_v_cap + eps))) * (beta1 * b_of_m_cap + (1 - beta1) * d_of_b[i] / (1 - np.power(beta1, update_iteration)))
        wandb.log({'epoch': iteration})
        print("Iteration :", iteration)
        training_Accuracy_Evaluation(no_of_layers, W_matrix, b_matrix, activationFunction, lossFunction)
        validation_Accuracy_Evaluation(no_of_layers, W_matrix, b_matrix, activationFunction, lossFunction)

###############################
# Model Training Function
###############################
def train_the_model(X_train, Y_train, no_of_hiddenlayers, no_of_neurons, no_of_input_nodes, no_of_output_nodes,
                    W_matrix, b_matrix, a_matrix, h_matrix, learningRate, no_of_epochs, batch_size,
                    no_of_training_samples, optimizer, activationFunction, lossFunction, initializer_type, Weight_decay):
    if optimizer == "sgd":
        _ = SchotasticGD_batch(no_of_hiddenlayers, no_of_neurons, no_of_epochs, Weight_decay, learningRate, batch_size, initializer_type, activationFunction, lossFunction)
    elif optimizer in ["momentum"]:
        _ = MomentumGD_batch(no_of_hiddenlayers, no_of_neurons, no_of_epochs, Weight_decay, learningRate, batch_size, initializer_type, activationFunction, lossFunction)
    elif optimizer in ["nag", "nesterov"]:
        _ = NesterovGD_batch(no_of_hiddenlayers, no_of_neurons, no_of_epochs, Weight_decay, learningRate, batch_size, initializer_type, activationFunction, lossFunction)
    elif optimizer == "rmsprop":
        _ = rmsProp_batch(no_of_hiddenlayers, no_of_neurons, no_of_epochs, Weight_decay, learningRate, batch_size, initializer_type, activationFunction, lossFunction)
    elif optimizer == "adam":
        _ = adam_batch(no_of_hiddenlayers, no_of_neurons, no_of_epochs, Weight_decay, learningRate, batch_size, initializer_type, activationFunction, lossFunction)
    elif optimizer == "nadam":
        _ = nadam_batch(no_of_hiddenlayers, no_of_neurons, no_of_epochs, Weight_decay, learningRate, batch_size, initializer_type, activationFunction, lossFunction)

###############################
# Command-Line Arguments and Main Function
###############################
import argparse

def get_args():
    parser = argparse.ArgumentParser(description="Deep Learning Model Training Arguments")
    
    # WandB arguments
    parser.add_argument(
        "-wp", "--wandb_project",
        type=str,
        default="myprojectname",
        help="Project name used to track experiments in Weights & Biases dashboard"
    )
    parser.add_argument(
        "-we", "--wandb_entity",
        type=str,
        default="myname",
        help="Wandb Entity used to track experiments in the Weights & Biases dashboard."
    )
    
    # Dataset argument
    parser.add_argument(
        "-d", "--dataset",
        type=str,
        choices=["mnist", "fashion_mnist"],
        default="fashion_mnist",
        help='Dataset to be used (\"mnist\" or \"fashion_mnist\").'
    )
    
    # Training hyperparameters
    parser.add_argument(
        "-e", "--epochs",
        type=int,
        default=10,
        help="Number of epochs to train neural network."
    )
    parser.add_argument(
        "-b", "--batch_size",
        type=int,
        default=64,
        help="Batch size used to train neural network."
    )
    parser.add_argument(
        "-l", "--loss",
        type=str,
        choices=["mean_squared_error", "cross_entropy"],
        default="cross_entropy",
        help="Loss function."
    )
    parser.add_argument(
        "-o", "--optimizer",
        type=str,
        choices=["sgd", "momentum", "nag", "rmsprop", "adam", "nadam"],
        default="nadam",
        help="Optimizer type."
    )
    parser.add_argument(
        "-lr", "--learning_rate",
        type=float,
        default=0.001,
        help="Learning rate used to optimize model parameters."
    )
    
    # Momentum & beta parameters
    parser.add_argument(
        "-m", "--momentum",
        type=float,
        default=0.9,
        help="Momentum used by momentum and nag optimizers."
    )
    parser.add_argument(
        "-beta", "--beta",
        type=float,
        default=0.9,
        help="Beta used by rmsprop optimizer."
    )
    parser.add_argument(
        "-beta1", "--beta1",
        type=float,
        default=0.999,
        help="Beta1 used by adam and nadam optimizers."
    )
    parser.add_argument(
        "-beta2", "--beta2",
        type=float,
        default=0.9,
        help="Beta2 used by adam and nadam optimizers."
    )
    parser.add_argument(
        "-eps", "--epsilon",
        type=float,
        default=0.000001,
        help="Epsilon used by optimizers."
    )
    
    # Regularization & initialization
    parser.add_argument(
        "-w_d", "--weight_decay",
        type=float,
        default=0.0005,
        help="Weight decay used by optimizers."
    )
    parser.add_argument(
        "-w_i", "--weight_init",
        type=str,
        choices=["random", "Xavier"],
        default="Xavier",
        help="Weight initialization method."
    )
    
    # Network structure
    parser.add_argument(
        "-nhl", "--num_layers",
        type=int,
        default=4,
        help="Number of hidden layers used in feedforward neural network."
    )
    parser.add_argument(
        "-sz", "--hidden_size",
        type=int,
        default=128,
        help="Number of hidden neurons in a feedforward layer."
    )
    parser.add_argument(
        "-a", "--activation",
        type=str,
        choices=["identity", "sigmoid", "tanh", "ReLU"],
        default="sigmoid",
        help="Activation function."
    )
    
    return parser.parse_args()


def main():
    args = get_args()
    print("Training Configuration:")
    for arg, value in vars(args).items():
        print(f"{arg}: {value}")

    # Load dataset based on the argument
    if args.dataset == "fashion_mnist":
        from keras.datasets import fashion_mnist
        (X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()
    else:
        from keras.datasets import mnist
        (X_train, Y_train), (X_test, Y_test) = mnist.load_data()

    # Optional: visualize some sample images
    global train_images, train_labels, validation_images, validation_labels
    train_images, validation_images, train_labels, validation_labels = train_test_split(X_train, Y_train, test_size=0.1)
    # Uncomment the following line to display sample images
    # plot_sample_images(X_train, Y_train)

    # Set additional parameters
    no_of_classes = 10
    input_size = 784
    no_of_training_samples = len(X_train)
    # Map loss function: cross_entropy -> entropy as used in Accuracy_Calculator
    if args.loss == "cross_entropy":
        loss_function = "entropy"
    else:
        loss_function = args.loss

    # Initialize Weights & Biases
    wandb.init(project=args.wandb_project, entity=args.wandb_entity)

    # Call the model training function with arguments from command line
    train_the_model(
        X_train, Y_train,
        args.num_layers,         # no_of_hiddenlayers
        args.hidden_size,        # no_of_neurons
        input_size,              # no_of_input_nodes
        no_of_classes,           # no_of_output_nodes
        {}, {}, {}, {},          # Dummy dictionaries; actual initialization happens in optimizer functions
        args.learning_rate,
        args.epochs,
        args.batch_size,
        no_of_training_samples,
        args.optimizer,
        args.activation,
        loss_function,
        args.weight_init,
        args.weight_decay
    )

if __name__ == "__main__":
    main()
